{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01 Tokenizing Words and Sentences with NLTK.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP9mKKbQNAwYf3L8pR6vnoO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanpaul21/NLP-using-Python/blob/master/01_Tokenizing_Words_and_Sentences_with_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF2cTw8nsB6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The NLTK module is a massive tool kit, \n",
        "# aimed at helping you with the entire Natural Language Processing (NLP) methodology. \n",
        "# NLTK will aid you with everything from splitting sentences from paragraphs, \n",
        "# splitting up words, recognizing the part of speech of those words, \n",
        "# highlighting the main subjects, and then even with helping your machine to understand \n",
        "# what the text is all about. "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrLyeRbPvadO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Corpus\n",
        "  # Body of text, singular. \n",
        "  # Corpora is the plural of this. \n",
        "  # Example: A collection of medical journals.\n",
        "\n",
        "# Lexicon\n",
        "  # Words and their meanings. \n",
        "  # Example: English dictionary. \n",
        "  # Consider, however, that various fields will have different lexicons. \n",
        "  # For example: To a financial investor, the first meaning for the word \"Bull\" \n",
        "  # is someone who is confident about the market, as compared to the common English lexicon,\n",
        "  # where the first meaning for the word \"Bull\" is an animal. \n",
        "  # As such, there is a special lexicon for financial investors, doctors, mechanics, etc.\n",
        "\n",
        "# Token\n",
        "  # Each \"entity\" that is a part of whatever was split up based on rules. \n",
        "  # For examples, each word is a token when a sentence is \"tokenized\" into words.\n",
        "  # Each sentence can also be a token, if you tokenized the sentences out of a paragraph."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hwMb4Fst232",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Libraries\n",
        "\n",
        "#!pip install nltk\n",
        "import nltk\n",
        "#nltk.download('punkt')\n",
        "from nltk import sent_tokenize, word_tokenize"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze52Vu4MwAsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First Step\n",
        "\n",
        "# We may think tokenizing by things like words/sentences,for many sentences it can be.\n",
        "# The first step would be likely doing a simple .split('.'), \n",
        "# or splitting by period followed by a space.\n",
        "# Then maybe wewould bring in some regular expressions to split \n",
        "# May be by period, space & then a capital letter.\n",
        "# The problem is that things like Mr. Paul would cause you trouble, and many other things. \n",
        "# Splitting by word is also a challenge,especially when considering things like concatenations \n",
        "# like we and are to we're. "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw3lZG1Otgb0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "acde658e-6344-43a9-ca13-66c88ca788ad"
      },
      "source": [
        "# Let tokenize by sentence,\n",
        "\n",
        "# Consider an Example\n",
        "EXAMPLE_TEXT = \"Hello Mr. Paul, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\"\n",
        "print(\"EXAMPLE :\")\n",
        "print(EXAMPLE_TEXT)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Sentence Tokenizer\n",
        "print(\"SENTENCE TOKENIZE :\")\n",
        "print(sent_tokenize(EXAMPLE_TEXT))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EXAMPLE :\n",
            "Hello Mr. Paul, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\n",
            "\n",
            "\n",
            "SENTENCE TOKENIZE :\n",
            "['Hello Mr. Paul, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPmdoLmMtzuB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "c6f68404-dd6f-403c-8b3f-5d0e660c639d"
      },
      "source": [
        "# Let tokenize by word instead this time:\n",
        "\n",
        "# Word Tokenizer\n",
        "print(\"WORD TOKENIZE\")\n",
        "print(word_tokenize(EXAMPLE_TEXT))\n",
        "\n",
        "# Inference : \n",
        "# There are a few things to note here. \n",
        "# First, notice that punctuation is treated as a separate token. \n",
        "# Also, notice the separation of the word \"shouldn't\" into \"should\" and \"n't.\" \n",
        "# Finally, notice that \"pinkish-blue\" is indeed treated like the \"one word\" \n",
        "# it was meant to be turned into."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WORD TOKENIZE\n",
            "['Hello', 'Mr.', 'Paul', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}