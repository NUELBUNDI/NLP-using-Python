{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyOZlOioxNc3NhuxB5vcVrvm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NUELBUNDI/NLP-using-Python/blob/master/Topic_Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Topic Modelling techniques\n",
        "\n",
        "1. Latent Smemantic Analysis (LSA)\n",
        "2. Probabilistic Latent Semantic Analysis (PLSA)\n",
        "3. Latent Dirichlet Allocation (LDA)\n",
        "4. Correlated Topic Model (CTM)\n",
        "\n",
        "\n",
        "\n",
        "LDA assumes that the documents are generated using a statistical generative process, such that each document is a mixture of topics, and each topics are a mixture of words."
      ],
      "metadata": {
        "id": "7OnoRyCkqmkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install pyLDAvis"
      ],
      "metadata": {
        "id": "EFRqdWieWbO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLn8wl05nzuX"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import nltk\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "\n",
        "# package to print\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Market review.csv',encoding='latin-1')"
      ],
      "metadata": {
        "id": "u4ZT_DC2rhHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning Methods\n",
        "\n",
        "1. Data Cleaning Methods\n",
        "\n",
        "    - lowercase\n",
        "    - tokenize\n",
        "    - remove stop words\n",
        "    - remove punctuation\n",
        "    - remove numbers\n",
        "    - remove white space\n",
        "    - remove special characters\n",
        "    - remove URLS\n",
        "    - remove Non-ASCII\n",
        "    - remove html tags\n",
        "    - lemmatize\n",
        "    - stem\n",
        "    - correct spelling\n",
        "    - remove diacritics\n",
        "    - remove emojis\n",
        "    - expand contractions\n",
        "    - case folding\n",
        "    - remove inconsistent whitespace\n",
        "    - spell checks and correction\n",
        "    - word filtering\n",
        "    - remove mark up languages\n",
        "    - detection of language"
      ],
      "metadata": {
        "id": "Npr2Cql-xJq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "# nltk.download(\"stopwords\")\n",
        "\n",
        "# Load the English language model in spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "import re"
      ],
      "metadata": {
        "id": "_P35VWpB3DpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "GgdNh9qa7HRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lowercasing\n",
        "def lowercase(text):\n",
        "  return text.lower()\n",
        "\n",
        "# Tokenize\n",
        "def tokenize(text):\n",
        "  return text.split()\n",
        "\n",
        "#Remove stopwords\n",
        "def remove_stopwords(text):\n",
        "  doc             = nlp(text)\n",
        "  tokens          = [token.text for token in doc]\n",
        "  filtered_tokens = [token for token in tokens if  token.lower() not in set(stopwords.words('english'))]\n",
        "  return ' '.join(filtered_tokens)\n",
        "\n",
        "#Punctuuation remova\n",
        "def removal_punctuation(text):\n",
        "  return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "# Number Removal\n",
        "def remove_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "#Special Character Removal\n",
        "def remove_special_characters(text):\n",
        "    return re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
        "\n",
        "#Whitespace Removal\n",
        "def remove_whitespace(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "# 11. Lemmatization (requires NLTK or spaCy)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def lemmatize(text, lemmatizer):\n",
        "    words = text.split()\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
        "\n",
        "# 12. Stemming (requires NLTK or spaCy)\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def stem(text, stemmer):\n",
        "    words = text.split()\n",
        "    return ' '.join([stemmer.stem(word) for word in words])\n",
        "\n",
        "#Word Filtering\n",
        "def filter_words(text, min_length=3, stopwords=[]):\n",
        "    words = text.split()\n",
        "    return ' '.join([word for word in words if len(word) >= min_length and word not in stopwords])\n",
        "\n"
      ],
      "metadata": {
        "id": "CTQSmkUOxId6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmer  = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "  text = lowercase(text)\n",
        "  # text = tokenize(text)\n",
        "  text = remove_stopwords(text)\n",
        "  text = removal_punctuation(text)\n",
        "  text = remove_numbers(text)\n",
        "  text = remove_special_characters(text)\n",
        "  text = remove_whitespace(text)\n",
        "  text = lemmatize(text,lemmer)\n",
        "  # text = stem(text,stemmer)\n",
        "\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "eOVcA0Zg4ri0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "9WhPMC2IZDgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach One"
      ],
      "metadata": {
        "id": "FlK04XvteoZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Clean Data"
      ],
      "metadata": {
        "id": "J9zDLjW-mLEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "df['data_words'] = list(sent_to_words(df.Market_Review))\n",
        "\n",
        "# After tokenization remove stoword and apply lemmatization\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['gikomba', 'however', 'still','various','also','increase', 'year','new','time'\n",
        "                    ])  #adding my own stop words\n",
        "\n",
        "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ','ADV']):\n",
        "\n",
        "    # remove stop words\n",
        "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "    texts_out = []\n",
        "\n",
        "    # Lemmatization\n",
        "    nlp = spacy.load(\"en_core_web_sm\",disable=['parser', 'ner'])\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "\n",
        "    # remove stopwords once more after lemmatization\n",
        "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]\n",
        "    return texts_out\n",
        "\n",
        "df['data_ready'] = process_words(df.data_words)  # processed Text Data!"
      ],
      "metadata": {
        "id": "Qr6OSMN1YqVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_words = df['data_ready'].values.tolist()"
      ],
      "metadata": {
        "id": "RtgQjcDwJBWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Build the bigram and trigram models\n",
        "# bigram     = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "# trigram    = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
        "# # Faster way to get a sentence clubbed as a trigram/bigram\n",
        "# bigram_mod  = gensim.models.phrases.Phraser(bigram)\n",
        "# trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "# # See trigram example\n",
        "# print(trigram_mod[bigram_mod[data_words[1]]])"
      ],
      "metadata": {
        "id": "v-Rcivn8IZdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model Building"
      ],
      "metadata": {
        "id": "k06u7ZiEmwdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Dictionary\n",
        "id2word = corpora.Dictionary(df.data_ready)\n",
        "\n",
        "#  Corpus: Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in df.data_ready]\n"
      ],
      "metadata": {
        "id": "uChe9zM_YqSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Coherence-"
      ],
      "metadata": {
        "id": "9cT_MfjFm02E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What is optimal Topics??\n",
        "\n",
        "Calculate Coherence score using C_umass"
      ],
      "metadata": {
        "id": "JLv1-_FRm9Wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from gensim.models import LdaMulticore\n",
        "topics = []\n",
        "score  = []\n",
        "for i in range(1,20,1):\n",
        "   lda_model = LdaMulticore(corpus=corpus, id2word=id2word, iterations=50, num_topics=i, workers = 4, passes=10, random_state=100)\n",
        "   cm        = CoherenceModel(model=lda_model, corpus=corpus, dictionary=id2word, coherence='u_mass')\n",
        "   topics.append(i)\n",
        "   score.append(cm.get_coherence())\n",
        "_=plt.plot(topics, score)\n",
        "_=plt.xlabel('Number of Topics')\n",
        "_=plt.ylabel('Coherence Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hx0OmfFfjj3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating the coherence score using C_v:"
      ],
      "metadata": {
        "id": "fCYXWKlBnKzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topics = []\n",
        "score  = []\n",
        "for i in range(1,20,1):\n",
        "   lda_model = LdaMulticore(corpus=corpus, id2word=id2word, iterations=10, num_topics=i, workers = 4, passes=10, random_state=100)\n",
        "   cm = CoherenceModel(model=lda_model, texts = df['data_ready'], corpus=corpus, dictionary=id2word, coherence='c_v')\n",
        "   topics.append(i)\n",
        "   score.append(cm.get_coherence())\n",
        "_=plt.plot(topics, score)\n",
        "_=plt.xlabel('Number of Topics')\n",
        "_=plt.ylabel('Coherence Score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bMxV1VjBlMDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimal Topics are 3 or 10"
      ],
      "metadata": {
        "id": "kDNN58XknTMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics= 10,\n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=20,\n",
        "                                           passes=100,\n",
        "                                           alpha='auto',\n",
        "                                           iterations=100,\n",
        "                                           per_word_topics=True)\n",
        "\n",
        "# Look at the topics and key words\n",
        "pprint(lda_model.print_topics())"
      ],
      "metadata": {
        "id": "4C1OHBqVnR9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Perplexity\n",
        "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=df.data_ready, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda      = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "metadata": {
        "id": "6midu-LxYqNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['Environ_Review'][0]"
      ],
      "metadata": {
        "id": "ZR2_B_MdnxrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lda_model[corpus][0]\n"
      ],
      "metadata": {
        "id": "yRJuMgron68Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, topic in lda_model.show_topics(formatted=False, num_words= 15):\n",
        "    print('Topic: {} --> Words: {}'.format(idx, '/'.join([w[0] for w in topic])))"
      ],
      "metadata": {
        "id": "UYbpKZ18YqKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the topic distribution\n",
        "\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "# Dominant topic in each review\n",
        "def topics_per_review(model, corpus, start=0, end=1):\n",
        "    corpus_sel = corpus[start:end]\n",
        "    dominant_topics = []\n",
        "    topic_percentages = []\n",
        "    for i, corp in enumerate(corpus_sel):\n",
        "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
        "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
        "        dominant_topics.append((i, dominant_topic))\n",
        "        topic_percentages.append(topic_percs)\n",
        "    return(dominant_topics, topic_percentages)\n",
        "\n",
        "dominant_topics, topic_percentages = topics_per_review(model=lda_model, corpus=corpus, end=-1)"
      ],
      "metadata": {
        "id": "7RKpn05zYqEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Dominant Topics in Each review\n",
        "ndf                           = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
        "dominant_topic_in_each_rev    = ndf.groupby('Dominant_Topic').size()\n",
        "df_dominant_topic_in_each_rev = dominant_topic_in_each_rev.to_frame(name='count').reset_index()\n",
        "display(df_dominant_topic_in_each_rev)"
      ],
      "metadata": {
        "id": "pBYL08fNYqCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total Topic Distribution by actual weight\n",
        "topic_weightage_by_rev     = pd.DataFrame([dict(t) for t in topic_percentages])\n",
        "df_topic_weightage_by_rev  = topic_weightage_by_rev.sum().to_frame(name='count').reset_index()\n",
        "\n",
        "display(df_topic_weightage_by_rev)"
      ],
      "metadata": {
        "id": "uNmCg3c8Yp_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.ticker import FuncFormatter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Top  Keywords for each Topic\n",
        "topic_top_n_words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False)\n",
        "                                 for j, (topic, wt) in enumerate(topics) if j < 5]  # for 5 key words\n",
        "\n",
        "df_top_n_words_stacked = pd.DataFrame(topic_top_n_words, columns=['topic_id', 'words'])\n",
        "df_top_n_words = df_top_n_words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
        "df_top_n_words.reset_index(level=0,inplace=True)\n",
        "\n",
        "# Plot\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 4), dpi=120, sharey=True)\n",
        "\n",
        "# Topic Distribution by Dominant Topics\n",
        "ax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_rev, width=.5, color='#9ECBEA')\n",
        "ax1.set_xticks(range(df_dominant_topic_in_each_rev.Dominant_Topic.unique().__len__()))\n",
        "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top_n_words.loc[df_top_n_words.topic_id==x, 'words'].values[0])\n",
        "# ax1.xaxis.set_major_formatter(tick_formatter)\n",
        "ax1.tick_params(labelsize=5)\n",
        "ax1.set_title('Number of Reviews by Dominant Topic', fontdict=dict(size=8))\n",
        "ax1.set_ylabel('Number of Reviews', fontsize = 8)\n",
        "ax1.set_ylim(0, 300)\n",
        "\n",
        "\n",
        "# Topic Distribution by Topic Weights\n",
        "ax2.bar(x='index', height='count', data=df_topic_weightage_by_rev, width=.5, color='#EADA9E')\n",
        "ax2.set_xticks(range(df_topic_weightage_by_rev.index.unique().__len__()))\n",
        "ax2.xaxis.set_major_formatter(tick_formatter)\n",
        "ax2.tick_params(labelsize=5)\n",
        "ax2.set_title('Number of Reviews by Topic Weightage', fontdict=dict(size=8))\n",
        "ax2.set_ylabel('Number of Review', fontsize = 8)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zYCUUJy5cBfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def format_topics_sentences(ldamodel=None, corpus=corpus, texts= df.Environ_Review):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "\n",
        "    for i, row_list in enumerate(ldamodel[corpus]):\n",
        "        row = row_list[0] if ldamodel.per_word_topics else row_list\n",
        "        # print(row)\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents         = pd.Series(texts)\n",
        "    contents.reset_index(drop=True, inplace=True)\n",
        "    sent_topics_df   = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "df_topic_sents_keywords   = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=df.Environ_Review)\n",
        "\n",
        "# Format\n",
        "df_dominant_topic         = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'review']\n",
        "display(df_dominant_topic.head(5))"
      ],
      "metadata": {
        "id": "-pJm1ie4cBcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dominant_topic.shape"
      ],
      "metadata": {
        "id": "ducB0Q1SCay5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.merge(df,df_dominant_topic, left_index=True, right_index=True)"
      ],
      "metadata": {
        "id": "D_F_bOpXCo5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final_df.to_csv('final_data.csv')"
      ],
      "metadata": {
        "id": "_t1r-OcDCVwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install pyLDAVis"
      ],
      "metadata": {
        "id": "fpdbDiklcBUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "pyLDAvis.enable_notebook()"
      ],
      "metadata": {
        "id": "uN7zUz8vAL97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install --upgrade pyLDAvis joblib pandas\n"
      ],
      "metadata": {
        "id": "M199NDHJpRvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reference\n",
        "\n",
        "1. https://github.com/rollyjohn/Topic-Modelling/blob/main/topic_model_V3.ipynb\n",
        "\n",
        "2. https://towardsdatascience.com/topic-modelling-in-python-with-spacy-and-gensim-dc8f7748bdbf"
      ],
      "metadata": {
        "id": "FzcR38GeCLUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### refe"
      ],
      "metadata": {
        "id": "Gg6RlfhbCIre"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}